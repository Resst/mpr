%%%% This is the file with a chapter body from the SPbPU-BCI template  %%%%
%%%% Filled with the article content about TCP and law of minimum

\renewcommand{\chapterEnTitle}{Application of the Law of Minimum for TCP}
\renewcommand{\chapterRuTitle}{Закон минимума и анализ различий для приоритизации регрессионных тестов}
\setcounter{mychapternumber}{1}

\input{template_settings/ch_epigraph} % keep unmodified except for \label{ch-11} after paper acceptance
\begin{abstr}
{\normalfont \abstractnameENG.}
We present the classical VIKOR multi-criteria decision-making method together with two important extensions: interval VIKOR and fuzzy VIKOR. We discuss their theoretical foundations, applicability, advantages and limitations, and provide comparative insight into decision stability under uncertainty. A computational example illustrates interval and fuzzy modifications of VIKOR evaluation and ranking.
\par
{\normalfont \keywordsENG.}
VIKOR, MCDM, interval analysis, fuzzy sets.
\par
{\normalfont \abstractname.}
В работе рассматривается классический метод многокритериального принятия решений VIKOR, а также две его ключевые модификации: интервальный и нечеткий VIKOR. Излагаются теоретические основы методов, области применимости, особенности и ограничения, а также проводится сравнение устойчивости решений в условиях неопределённости. На иллюстративном примере демонстрируется выполнение и результаты интервальной и нечеткой модификации VIKOR.
\par
{\normalfont \keywords.}
VIKOR, многокритериальное принятие решений, интервальный анализ, нечеткие множества.
\end{abstr}

\begin{refsection}[my_folder/my_biblio.bib]

\section{Введение}

Метод VIKOR (\emph{VIseKriterijumska Optimizacija I Kompromisno Resenje}) был предложен Сергием Опричевичем как подход к многокритериальному принятию решений, ориентированный на нахождение \emph{компромиссного решения}, максимально близкого к идеальной точке. В отличие от методов, стремящихся к парето-оптимальным решениям, VIKOR фокусируется на ситуациях, где компромисс между критериями является предпочтительным.

Однако классическая версия метода предполагает, что значения критериев заданы в виде точных чисел. В реальных задачах часто присутствуют различные формы неопределённости: интервальные оценки показателей, экспертные оценки в форме языковых термов, нечеткие треугольные числа и др. Это приводило к необходимости расширить VIKOR.

Существуют две важные модификации:
\begin{itemize}
    \item \textbf{интервальный VIKOR}, предложенный в работе \cite{sayadi2009extension}, адаптирующий алгоритм к критериям, представленным через интервалы;
    \item \textbf{fuzzy VIKOR}, описанный, например, в работах \cite{wan2013extended, liu2017extended}, позволяющий работать с лингвистическими и нечеткими данными.
\end{itemize}

Сравнительный анализ различных версий VIKOR выполнен в статье \cite{chatterjee2016comparative}, где показано, что интервальная и нечеткая модификации позволяют повысить устойчивость решений при наличии неопределенности.

Цель данной работы — систематизировать основные идеи интервальной и нечеткой модификаций VIKOR, показать отличие от классического алгоритма и продемонстрировать работу интервального и нечеткого VIKOR на примере, написанном на языке программирования python.

\section{Обзор метода VIKOR}\label{orig_vikor}

Метод VIKOR относится к классу многокритериальных подходов принятия решений и был предложен Опричевичем как способ получения компромиссного решения в ситуациях, где необходимо учитывать несколько конфликтующих критериев. Первоначально метод создавался для инженерных задач, требующих выбора оптимального варианта среди множества альтернатив с разнородными характеристиками. Основная идея метода заключается в нахождении решения, минимизирующего расстояние до идеальной точки и одновременно обеспечивающего баланс между улучшением средних показателей и уменьшением максимального отклонения. Метод основан на компромиссном программировании, но отличается тем, что явно использует процедуру поиска решения, приемлемого для большинства заинтересованных сторон, что делает его особенно полезным в задачах, предполагающих наличие противоречивых критериев.

VIKOR применяется в ситуациях, где требуется учитывать не только максимальную близость альтернативы к идеальному варианту, но и степень её доминирования по самому неблагоприятному критерию. Поэтому метод получил широкое применение в инженерии, инвестиционном анализе, экологии, управлении проектами и других областях, где невозможно достичь одновременной оптимизации всех целей. Преимуществом метода является способность сочетать стратегии минимизации наихудшего отклонения и максимизации интегральной пользы, что позволяет получать взвешенное компромиссное решение.

\subsection{Математическая постановка задачи}

Пусть задано множество альтернатив \(A = \{A_1, A_2, \dots, A_m\}\), каждая из которых оценивается по набору критериев \(C = \{c_1, c_2, \dots, c_n\}\). Определена матрица оценок \(f_{ij}\), где \(f_{ij}\) — значение альтернативы \(A_i\) по критерию \(c_j\). Для каждого критерия задан тип: выгодный (benefit), для которого большие значения предпочтительны, или затратный (cost), для которого предпочтительны меньшие значения. Также каждому критерию назначен весовой коэффициент \(w_j\), удовлетворяющий условию
\[
\sum_{j=1}^{n} w_j = 1.
\]

Для каждого критерия определяются лучшие и худшие значения среди всех альтернатив. Для выгодных критериев применяются выражения
\[
f_j^{*} = \max_{i} f_{ij}, \qquad f_j^{-} = \min_{i} f_{ij},
\]
а для затратных критериев
\[
f_j^{*} = \min_{i} f_{ij}, \qquad f_j^{-} = \max_{i} f_{ij}.
\]

Затем вычисляются две меры отклонения альтернативы от идеального решения. Первая мера представляет собой взвешенную сумму отклонений:
\[
S_i = \sum_{j=1}^{n} w_j \,\frac{f_j^{*} - f_{ij}}{f_j^{*} - f_j^{-}}.
\]
Вторая мера характеризует максимальное взвешенное отклонение по одному критерию:
\[
R_i = \max_{1\le j\le n} \left\{ w_j \,\frac{f_j^{*} - f_{ij}}{f_j^{*} - f_j^{-}} \right\}.
\]

Для получения компромиссного решения определяется интегральный индекс \(Q_i\):
\[
Q_i = v \cdot \frac{S_i - S^{*}}{S^{-} - S^{*}} + (1-v) \cdot \frac{R_i - R^{*}}{R^{-} - R^{*}},
\]
где
\[
S^{*} = \min_{i} S_i,\quad S^{-} = \max_{i} S_i,\qquad R^{*} = \min_{i} R_i,\quad R^{-} = \max_{i} R_i,
\]
а параметр \(v \in [0,1]\) отражает предпочтения относительно стратегий компромисса. Значение \(v = 0.5\) соответствует равному учёту обеих мер, приближение \(v \to 1\) подчеркивает ориентацию на минимизацию суммарных отклонений, а \(v \to 0\) — на минимизацию наибольшего отклонения.

Итоговое решение определяется альтернативой с минимальным значением \(Q_i\), однако процедура выбора включает дополнительные условия устойчивости. Если различие между лучшими альтернативами недостаточно велико или ранжирование по метрикам \(S\) и \(R\) противоречит индексу \(Q\), формируется не одна альтернатива, а набор компромиссных решений.

\section{Интервальная модификация метода VIKOR}

Как уже упоминалось в \ref{orig_vikor}, классический метод VIKOR предполагает, что оценки всех альтернатив по каждому
критерию представлены точечными значениями. Однако в реальных задачах
многокритериального анализа нередко встречаются ситуации, когда значения
критериев заданы неточно, являются приближёнными или варьируются в некотором
допустимом диапазоне. Такие ситуации типичны для инженерных,
экономических и экологических задач, в которых невозможно получить точные числа
из-за экспертной неопределённости, вариативности данных и измерительных ошибок.

Для учёта неопределённости авторы \cite{sayadi2009} предложили
интервальную модификацию метода VIKOR, в которой вместо точечных значений
используются интервальные оценки
\[
f_{ij} = [\, \underline{f}_{ij},\; \overline{f}_{ij} \,],
\]
где $\underline{f}_{ij}$ и $\overline{f}_{ij}$ обозначают нижнюю и верхнюю границы
возможного значения критерия. Такой подход позволяет работать с более гибкой
моделью данных и принимать решения, устойчивые к колебаниям входной информации.

\subsection{Математическая постановка интервального VIKOR}

Пусть задано множество альтернатив $A = \{A_1, \dots, A_m\}$, множество критериев
$C = \{c_1, \dots, c_n\}$ и интервальная матрица оценок
\[
f_{ij} = [\, \underline{f}_{ij},\; \overline{f}_{ij} \,].
\]
Каждый критерий относится к типу \emph{выгодный} или \emph{затратный}, а веса
$w_j$ удовлетворяют условию $\sum_{j=1}^{n} w_j = 1$.

Для интервальных данных определяются \emph{интервальные идеальные} и
\emph{антиидеальные точки}. Для выгодного критерия:
\[
f_j^{*} = 
\left[
\max_i \underline{f}_{ij},\;
\max_i \overline{f}_{ij}
\right], \qquad
f_j^{-} =
\left[
\min_i \underline{f}_{ij},\;
\min_i \overline{f}_{ij}
\right].
\]
Для затратного критерия:
\[
f_j^{*} = 
\left[
\min_i \underline{f}_{ij},\;
\min_i \overline{f}_{ij}
\right], \qquad
f_j^{-} =
\left[
\max_i \underline{f}_{ij},\;
\max_i \overline{f}_{ij}
\right].
\]

Далее необходимо определить интервальные меры отклонения. Интервальный
нормализованный разрыв альтернативы $A_i$ по критерию $c_j$ определяется как
\[
D_{ij} = 
\left[
\frac{f_j^{*\,(\text{low})} - \overline{f}_{ij}}{f_j^{*\,(\text{low})} - f_j^{-\,(\text{high})}},
\quad
\frac{f_j^{*\,(\text{high})} - \underline{f}_{ij}}{f_j^{*\,(\text{high})} - f_j^{-\,(\text{low})}}
\right],
\]
где использование противоположных концов интервалов обеспечивает
\emph{наиболее пессимистичную} и \emph{наиболее оптимистичную} оценки,
согласуясь с подходом Sayadi et al.

Интервальные показатели $S_i$ и $R_i$ вычисляются как:
\[
S_i =
\left[
\sum_{j=1}^{n} w_j\, D_{ij}^{(\text{low})},
\quad
\sum_{j=1}^{n} w_j\, D_{ij}^{(\text{high})}
\right],
\]
\[
R_i =
\left[
\max_{j} w_j\, D_{ij}^{(\text{low})},
\quad
\max_{j} w_j\, D_{ij}^{(\text{high})}
\right].
\]


\subsection{Особенности метода}

Интервальный VIKOR сохраняет структуру исходного метода, но адаптирует
все вычисления к интервальной арифметике. Основные особенности:

\begin{itemize}
	\item вместо точек используются интервалы значений, что делает метод чувствительным к неопределённости;
	\item нормализация использует ``наиболее широкое'' сочетание концов интервалов, что гарантирует корректную оценку в условиях неполных данных;
	\item результаты ранжирования также представляются интервалами и требуют процедуры сравнения интервальных чисел;
	\item метод может порождать частичные или нестрогие порядки, когда интервалы $Q_i$ альтернатив пересекаются.
\end{itemize}

\subsection{Недостатки и ограничения}

Несмотря на преимущества, интервальная модификация имеет ряд недостатков:

\begin{itemize}
	\item усложнение вычислений из-за необходимости поддерживать интервальную арифметику;
	\item увеличение числа случаев, когда альтернативы оказываются несравнимыми из-за пересечения интервалов;
	\item зависимость качества результата от корректного задания интервалов экспертами.
\end{itemize}

Тем не менее модификация доказала свою эффективность в задачах с высокой неопределённостью и стала основой для дальнейших расширений метода, включая fuzzy модификацию \cite{liu2017, wan2013}, о которой пойдет речь в \ref{fuzzy_modification}.


\subsection{Нечёткая (fuzzy) модификация метода VIKOR} \label{fuzzy_modification}

В ряде практических задач исходные данные содержат неопределённость, которую невозможно корректно описать только интервалами. В таких случаях применяется нечеткая (fuzzy) модификация метода VIKOR, предложенная в~\cite{OPRICOVIC201112983}. В этой версии критерии и веса допускают неопределённость, выражаемую \emph{треугольными нечеткими числами} (TFN). Нечеткие оценки позволяют моделировать неточность экспертных суждений, а также различать нижнюю, наиболее вероятную и верхнюю оценку каждого параметра.

Треугольное нечеткое число определяется как
\[
\tilde{x} = \bigl(x^{\mathrm{L}},\, x^{\mathrm{M}},\, x^{\mathrm{U}}\bigr),
\]
где $x^{\mathrm{L}}$, $x^{\mathrm{M}}$ и $x^{\mathrm{U}}$ соответствуют нижней, модальной и верхней оценкам соответственно. Все операции в алгоритме выполняются покомпонентно, что обеспечивает корректность результата в нечеткой среде.

\subsubsection{Определение нечетких идеального и антиидеального решений}

Аналогично классическому VIKOR для каждого критерия определяются лучшие и худшие значения. Отличие заключается в том, что каждая оценка является TFN, а операции $\max$ и $\min$ применяются к соответствующим компонентам.

Для выгодных критериев:
\[
\tilde{f}_j^{*} = 
\left(
\max_i f_{ij}^{\mathrm{L}},
\max_i f_{ij}^{\mathrm{M}},
\max_i f_{ij}^{\mathrm{U}}
\right),
\qquad
\tilde{f}_j^{-} = 
\left(
\min_i f_{ij}^{\mathrm{L}},
\min_i f_{ij}^{\mathrm{M}},
\min_i f_{ij}^{\mathrm{U}}
\right).
\]

Для затратных критериев:
\[
\tilde{f}_j^{*} = 
\left(
\min_i f_{ij}^{\mathrm{L}},
\min_i f_{ij}^{\mathrm{M}},
\min_i f_{ij}^{\mathrm{U}}
\right),
\qquad
\tilde{f}_j^{-} = 
\left(
\max_i f_{ij}^{\mathrm{L}},
\max_i f_{ij}^{\mathrm{M}},
\max_i f_{ij}^{\mathrm{U}}
\right).
\]

\subsubsection{Нормирование нечетких расстояний}

Нечеткое расстояние альтернативы от идеального решения также является треугольным числом. В работе~\cite{OPRICOVIC201112983} предложена нормировка вида
\[
\tilde{d}_{ij} =
\left(
\frac{f_j^{\*\,\mathrm{L}} - f_{ij}^{\mathrm{U}}}{f_j^{\*\,\mathrm{U}} - f_j^{-\,\mathrm{L}}},
\;
\frac{f_j^{\*\,\mathrm{M}} - f_{ij}^{\mathrm{M}}}{f_j^{\*\,\mathrm{M}} - f_j^{-\,\mathrm{M}}},
\;
\frac{f_j^{\*\,\mathrm{U}} - f_{ij}^{\mathrm{L}}}{f_j^{\*\,\mathrm{L}} - f_j^{-\,\mathrm{U}}}
\right),
\]
что гарантирует сохранение структуры TFN.

\subsubsection{Нечеткие показатели $S_i$ и $R_i$}

Аналоги интегральной и максимальной меры отклонения определяются как

\[
\tilde{S}_i = 
\sum_{j=1}^n 
\bigl( \tilde{w}_j \otimes \tilde{d}_{ij} \bigr),
\]

\[
\tilde{R}_i = 
\max_j 
\bigl( \tilde{w}_j \otimes \tilde{d}_{ij} \bigr),
\]

где $\otimes$ обозначает покомпонентное умножение TFN, а максимум по критериям также определяется покомпонентно:
\[
\max\bigl((a^{\mathrm{L}},a^{\mathrm{M}},a^{\mathrm{U}}),(b^{\mathrm{L}},b^{\mathrm{M}},b^{\mathrm{U}})\bigr)
=
\bigl(
\max(a^{\mathrm{L}},b^{\mathrm{L}}),
\max(a^{\mathrm{M}},b^{\mathrm{M}}),
\max(a^{\mathrm{U}},b^{\mathrm{U}})
\bigr).
\]

\subsubsection{Дефаззификация и вычисление итогового показателя}

Поскольку результатом являются нечеткие числа, необходимо дефаззифицировать $\tilde{S}_i$ и $\tilde{R}_i$. В~\cite{OPRICOVIC201112983} применяется классическая дефаззификация треугольного числа:
\[
\mathrm{Defuzz}(\tilde{x}) = \frac{x^{\mathrm{L}} + x^{\mathrm{M}} + x^{\mathrm{U}}}{3}.
\]

Таким образом,
\[
S_i = \mathrm{Defuzz}(\tilde{S}_i), 
\qquad 
R_i = \mathrm{Defuzz}(\tilde{R}_i).
\]

После этого итоговый компромиссный показатель вычисляется по стандартной формуле VIKOR:
\[
Q_i = 
v\,\frac{S_i - S^{*}}{S^{-} - S^{*}}
+
(1-v)\,\frac{R_i - R^{*}}{R^{-} - R^{*}},
\]
где
\[
S^{*} = \min_i S_i,\quad 
S^{-} = \max_i S_i,\qquad
R^{*} = \min_i R_i,\quad 
R^{-} = \max_i R_i,
\]
а $v \in [0,1]$ — параметр компромисса.


\subsubsection{Итоговое ранжирование}

Окончательные решения выбираются по тем же критериям приемлемости преимущества и стабильности, что и в оригинальном методе VIKOR. Отличие заключается только в том, что все сравнения проводятся над дефаззифицированными значениями.


\section{Обзор существующих подходов к TCP}

Задача приоритизации тестовых случаев активно исследуется на протяжении
последних десятилетий. Наиболее распространёнными являются следующие
классы методов.

\subsection{Coverage-based методы}

Coverage-based TCP \-методы используют информацию о покрытии кода
тестами. Предполагается, что тесты, покрывающие больше кода (особенно
изменённого), с большей вероятностью обнаружат дефекты.

Классический \textbf{Total} метод сортирует тесты по убыванию общего
покрытия~--- в первую очередь запускаются тесты, покрывающие наибольшее
число элементов (строк, ветвей, методов и т.~п.). Аналогично,
\textbf{Total-Diff} фокусируется на покрытии именно изменённых
(impacted) элементов.

Метод \textbf{Additional} выбирает тесты итеративно: на каждом шаге
выбирается тест, добавляющий максимальное количество \emph{ещё не
покрытых} элементов. Его дифф-ориентированный вариант
\textbf{Additional-Diff} работает только с множеством изменённых
элементов программы.

Преимуществом таких методов является простота и широкая поддержка
инструментами покрытия. Однако они не учитывают структуру зависимостей
между компонентами и предполагают, что все покрытые элементы одинаково
важны.

\subsection{Diff-based методы}

Diff-based подходы используют информацию о различиях между версиями
программы: на основе diff (например, по байт-коду) выделяются
изменённые файлы, классы или методы. Далее тесты сортируются по тому,
какое количество изменённых элементов они покрывают.

Методы Total-Diff и Additional-Diff относятся к этому классу, но
существуют и более сложные варианты, интегрирующие дифф-информацию с
историей предыдущих запусков тестов, покрытием и другими факторами.

\subsection{History-based методы}

Исторические методы приоритизации используют информацию о прошлых
запусках тестов: например, тесты, чаще всего обнаруживавшие дефекты в
прошлых версиях, могут запускаться раньше. Такие подходы хорошо работают
при наличии длинной истории изменений и запусков, но малоприменимы для
новых проектов.

\subsection{Ограничения традиционных подходов}

Основные ограничения классических методов TCP можно сформулировать
следующим образом:
\begin{itemize}
  \item отсутствие моделирования \emph{распространения изменений} через
        граф вызовов: изменение в одном методе может косвенно повлиять
        на поведение множества других методов;
  \item равное отношение ко всем изменённым элементам без учёта того,
        насколько логика метода зависит от его входных параметров;
  \item игнорирование \emph{похожести} тестов: два теста с почти
        идентичным покрытием вносят мало дополнительной информации,
        но могут выполняться подряд.
\end{itemize}

Для преодоления этих ограничений в рассматриваемой статье предложено
объединить анализ потока управления, анализ различий, модель цепи
Маркова и закон минимума.

\section{Методология работы}

\subsection{Общая архитектура анализа}

Предлагаемый в статье подход можно представить как конвейер, включающий
несколько этапов:
\begin{enumerate}
  \item анализ изменений (diff) на уровне байт-кода и вычисление
        \emph{ratio of change} для методов;
  \item построение графа вызовов (Call Graph) и графов потока управления
        (Control Flow Graph, CFG);
  \item выполнение forward slicing по CFG и оценка чувствительности методов к параметрам;
  \item построение цепи Маркова на множестве методов и вычисление
        вектора влияния (impact vector);
  \item вычисление метрик LoM-Score и Dis-LoM-Score для тестов;
  \item упорядочивание тестов в соответствии с полученными метриками.
\end{enumerate}

В качестве инструментов используются:
\begin{itemize}
  \item \textbf{Soot}~--- фреймворк для анализа Java-байткода, позволяющий строить CFG;
  \item \textbf{java-callgraph}~--- утилита для построения графа вызовов;
  \item \textbf{reJ}~--- инструмент для сравнения байт-кода и оценки \emph{ratio of change}.
\end{itemize}

\subsection{Forward slicing и оценка чувствительности}

Для каждого метода программы строится граф потока управления~--- CFG.
Вершины CFG соответствуют отдельным инструкциям или операторам, а рёбра
обозначают возможные переходы управления (следующая инструкция,
ветвления, выход из цикла и т.~п.).

Далее выполняется \emph{forward slicing} относительно параметров метода.
На вход slicing-процедуры подаются:
\begin{itemize}
  \item начальные узлы (источники)~--- параметры метода;
  \item CFG рассматриваемого метода.
\end{itemize}

Алгоритм находит все инструкции, которые напрямую используют параметры,
а затем распространяет зависимость вперёд по CFG, учитывая передачу
значений через переменные. В результате получается подграф
$pCFG_{m_i} \subseteq CFG_{m_i}$, содержащий только те инструкции,
которые зависят от параметров метода $m_i$.

На основе этого вводится метрика чувствительности метода к параметрам:
\[
P(m_i) = \frac{|pCFG_{m_i}|}{|CFG_{m_i}|},
\]
где $|pCFG_{m_i}|$~--- количество вершин в подграфе, а $|CFG_{m_i}|$~---
общее количество вершин CFG. Чем выше $P(m_i)$, тем большая часть тела
метода зависит от его параметров, и тем потенциально сильнее влияние
изменений входных данных на поведение метода.

\subsection{Модель цепи Маркова}

Следующий элемент методологии~--- построение модели цепи Маркова на
множество методов. Состояния цепи соответствуют методам программы, а
переходы описывают возможное распространение влияния изменений по графу
вызовов.

Пусть метод $A$ вызывает метод $B$. Тогда в матрице переходов $T$
вводится элемент $T_{AB} > 0$. Вес перехода может зависеть, в частности,
от чувствительности метода $B$ (значения $P(B)$). Далее веса для каждого
состояния нормируются таким образом, чтобы сумма по строке была равна
единице, то есть матрица $T$ становится стохастической.

Для инициализации цепи используется вектор $I$, основанный на данных
diff. Инструмент reJ позволяет оценить \emph{ratio of change} для
каждого метода:
\[
roc(m_i) = \frac{\#\text{изменённых инструкций}}{\#\text{всех инструкций}}.
\]
После нормирования значений $roc(m_i)$ получается начальный вектор
$I$, отражающий исходную ``силу'' изменений.

Вектор влияния (impact vector) определяется как:
\[
Impact = I \times T,
\]
где произведение понимается в обычном матричном смысле. Компонента
$Impact(m_i)$ интерпретируется как вероятность того, что метод $m_i$
будет затронут изменениями напрямую или через транзитивные зависимости.

\subsection{Метрика LoM-Score}

Основой LoM-Score является \emph{закон минимума}, известный из биологии
(закон Либиха): рост организма ограничен фактором, находящимся в
дефиците. Перенося эту идею на тестирование, можно сказать, что
эффективность теста ограничивается его ``самым слабым'' покрытием.

Для каждого теста $t$ рассматривается множество покрываемых им методов
$M_t$. Для всех $m \in M_t$ берутся значения $Impact(m)$. Далее
выполняются следующие шаги:
\begin{enumerate}
  \item значения $Impact(m)$ сортируются по возрастанию;
  \item из отсортированного набора выбирается нижний квартиль
        (first quartile, $Q_1$);
  \item LoM-Score определяется как среднее значение элементов
        нижнего квартиля.
\end{enumerate}

Таким образом, если среди покрываемых тестом методов есть ``слабые''
с точки зрения вероятности влияния, LoM-Score будет ниже. Тесты с
высоким LoM-Score считаются более перспективными для раннего запуска.

\subsection{Метрика Dis-LoM-Score}

Несмотря на полезность LoM-Score, возможна ситуация, когда несколько
тестов с высокими значениями LoM имеют почти одинаковое покрытие
методов. Запуск таких тестов подряд приводит к дублированию работы и
задержке обнаружения дефектов в других частях системы.

Для учёта непохожести тестов вводится Dis-LoM-Score. Для оценки
сходства двух тестов $t_i$ и $t_j$ используется коэффициент Жаккара:
\[
Sim(t_i, t_j) =
\frac{|Cover_i \cap Cover_j|}{|Cover_i \cup Cover_j|},
\]
где $Cover_i$ и $Cover_j$~--- множества методов, покрываемых
соответствующими тестами.

При выборе следующего теста в порядке запуска для теста $t$
рассчитывается:
\[
DisLoM(t) = LoM(t) \cdot \bigl(1 - \max_{t' \in Selected} Sim(t, t')\bigr),
\]
где $Selected$~--- множество уже выбранных тестов. Тесты, сильно
похожие на уже выполненные (имеющие высокое сходство по Жаккару),
получают дополнительный штраф и смещаются вниз в порядке исполнения.

\section{Экспериментальная часть}

\subsection{Данные и настройки экспериментов}

Экспериментальная оценка метода проведена авторами статьи на основе
набора Defects4J, включающего реальные Java-проекты с искусственно
сохранёнными дефектными версиями. В частности, рассматривались
следующие проекты:
\begin{itemize}
  \item Jsoup, Gson, Csv;
  \item JacksonDatabind, JacksonXml, JacksonCore;
  \item Compress, Time, Codec, Lang.
\end{itemize}

Для генерации искусственных дефектов применялось мутационное
тестирование с использованием инструмента Major. Качество различных
стратегий приоритизации оценивалось по метрике APFD (Average Percentage
of Faults Detected), показывающей, насколько быстро по порядку тестов
обнаруживаются дефекты.

В качестве базовых методов сравнения (baseline) использовались:
\begin{itemize}
  \item Total и Additional;
  \item Total-Diff и Additional-Diff;
  \item случайные порядки (Random).
\end{itemize}

\subsection{Результаты для LoM и Dis-LoM}

В таблице~\ref{tab:lom-dis} приведены значения APFD для методов LoM и
Dis-LoM для части проектов Defects4J (по данным оригинальной статьи).

\begin{table}[h]
  \centering
  \caption{Сравнение LoM и Dis-LoM по APFD}
  \label{tab:lom-dis}
  \begin{tabular}{lccc}
    \toprule
    Проект & LoM & Dis-LoM & $\Delta$ \\
    \midrule
    Csv             & 0{,}9643 & 0{,}9643 & 0{,}0000 \\
    JacksonDatabind & 0{,}9921 & 0{,}9924 & +0{,}0003 \\
    JacksonXml      & 0{,}8795 & 0{,}9399 & +0{,}0604 \\
    Time            & 0{,}9791 & 0{,}9915 & +0{,}0124 \\
    Codec           & 0{,}7549 & 0{,}9540 & +0{,}1991 \\
    JacksonCore     & 0{,}9715 & 0{,}9729 & +0{,}0014 \\
    Lang            & 0{,}9938 & 0{,}9938 & 0{,}0000 \\
    \bottomrule
  \end{tabular}
\end{table}

Можно отметить, что Dis-LoM никогда не показывает результаты хуже, чем
LoM. На некоторых проектах (Csv, Lang) значения APFD совпадают, на
других наблюдается небольшое улучшение. Наиболее заметный прирост
достигается на проектах Codec и JacksonXml, где Dis-LoM значительно
смещает полезные тесты вверх по порядку, устраняя дублирование покрытия.

\subsection{Сравнение с базовыми методами}

В среднем по рассматриваемым проектам значения APFD для различных
методов приоритизации распределились следующим образом (по данным
статьи):

\begin{table}[h]
  \centering
  \caption{Средние значения APFD для разных методов}
  \label{tab:apfd-average}
  \begin{tabular}{lc}
    \toprule
    Метод           & Средний APFD \\
    \midrule
    Additional-Diff & 0{,}9805 \\
    Dis-LoM         & 0{,}9727 \\
    Total-Diff      & 0{,}9625 \\
    LoM             & 0{,}9336 \\
    Additional      & 0{,}8656 \\
    Random (avg)    & 0{,}8236 \\
    Total           & 0{,}8062 \\
    \bottomrule
  \end{tabular}
\end{table}

Из таблицы~\ref{tab:apfd-average} видно, что методы, учитывающие diff
(Additional-Diff, Total-Diff), демонстрируют более высокие значения
APFD по сравнению с классическими Total и Additional. Предложенный
метод Dis-LoM оказывается близок к лучшему baseline
Additional-Diff и превосходит остальные методы, в том числе стандартные
coverage-based и случайный порядок.

\subsection{Краткий анализ результатов}

Результаты показывают, что:
\begin{itemize}
  \item интеграция информации о распространении изменений (через цепь
        Маркова) и закона минимума позволяет более точно оценивать
        важность тестов по отношению к изменённому коду;
  \item учёт различий между тестами (Dis-LoM) уменьшает дублирование и
        ускоряет обнаружение дефектов в разных частях системы;
  \item Dis-LoM демонстрирует стабильное улучшение по сравнению с LoM и
        конкурентоспособен с лучшими diff-ориентированными baseline.
\end{itemize}

\section{Выводы}

В работе рассмотрен подход к приоритизации регрессионных тестов,
основанный на сочетании анализа различий, закона минимума и модели
цепей Маркова. В отличие от классических coverage-based методов, данный
подход:
\begin{itemize}
  \item моделирует распространение влияния изменений по графу вызовов;
  \item учитывает чувствительность методов к параметрам через forward slicing;
  \item опирается на закон минимума для выявления ``узких мест'' в покрытии;
  \item снижает избыточность тестов за счёт анализа непохожести покрытия.
\end{itemize}

Экспериментальные результаты, полученные авторами исходной статьи на
наборе Defects4J, показывают, что метрика Dis-LoM обеспечивает высокие
значения APFD и сопоставима с лучшими diff-ориентированными методами,
превосходя при этом классические Total и Additional, а также случайный
порядок.

Перспективными направлениями развития подхода являются:
\begin{itemize}
  \item расширение анализа на другие языки программирования и типы проектов;
  \item интеграция исторической информации о сбоях тестов;
  \item оптимизация вычислительной стоимости построения графов и
        модели цепи Маркова для очень больших систем.
\end{itemize}

\section*{Библиографический список}
\printbibliography
\end{refsection}
\newpage % keep unmodified
